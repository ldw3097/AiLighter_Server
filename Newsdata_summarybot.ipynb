{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wv5f3XOLpZ7o"
      },
      "outputs": [],
      "source": [
        "# Bertsum directory chdir\n",
        "os.chdir('/content/KoBertSum/src')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3Uzgqx3cLy0"
      },
      "source": [
        "#3. BERT forward propagation workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3m9X2IApKrt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "    Main training workflow\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "\n",
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "import signal\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from pytorch_pretrained_bert import BertConfig\n",
        "\n",
        "\n",
        "import distributed\n",
        "from models import data_loader, model_builder\n",
        "from models.data_loader import load_dataset\n",
        "from models.model_builder import Summarizer\n",
        "from tensorboardX import SummaryWriter\n",
        "from models.reporter import ReportMgr\n",
        "from models.stats import Statistics\n",
        "from others.logging import logger\n",
        "# from models.trainer import build_trainer\n",
        "# build_trainer의 dependency package pyrouge.utils가 import되지 않아 직접 셀에 삽입\n",
        "from others.logging import logger, init_logger\n",
        "import easydict\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "    \"encoder\":'classifier',\n",
        "    \"mode\":'summary',\n",
        "    \"bert_data_path\":'/home/dongwon/korbertsum/bert_data/korean',\n",
        "    \"model_path\":'/home/dongwon/korbertsum/models/bert_classifier' ,\n",
        "    \"bert_model\":'/home/dongwon/001_bert_morp_pytorch',\n",
        "    \"result_path\":'/home/dongwon/korbertsum/results/korean',\n",
        "    \"temp_dir\":'.',\n",
        "    \"bert_config_path\":'/home/dongwon/001_bert_morp_pytorch/bert_config.json',\n",
        "    \"batch_size\":1000,\n",
        "    \"use_interval\":True,\n",
        "    \"hidden_size\":128,\n",
        "    \"ff_size\":512,\n",
        "    \"heads\":4,\n",
        "    \"inter_layers\":2,\n",
        "    \"rnn_size\":512,\n",
        "    \"param_init\":0,\n",
        "    \"param_init_glorot\":True,\n",
        "    \"dropout\":0.1,\n",
        "    \"optim\":'adam',\n",
        "    \"lr\":2e-3,\n",
        "    \"report_every\":1,\n",
        "    \"save_checkpoint_steps\":5,\n",
        "    \"block_trigram\":True,\n",
        "    \"recall_eval\":False,\n",
        "    \n",
        "    \"accum_count\":1,\n",
        "    \"world_size\":1,\n",
        "    \"visible_gpus\":'-1',\n",
        "    \"gpu_ranks\":'0',\n",
        "    \"log_file\":'/home/dongwon/korbertsum/logs/bert_classifier',\n",
        "    \"test_from\":'/home/dongwon/korbertsum/models/bert_classifier/model_step_1000.pt'\n",
        "})\n",
        "\n",
        "\n",
        "def build_trainer(args, device_id, model,\n",
        "                  optim):\n",
        "    \"\"\"\n",
        "    Simplify `Trainer` creation based on user `opt`s*\n",
        "    Args:\n",
        "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
        "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
        "        fields (dict): dict of fields\n",
        "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
        "        data_type (str): string describing the type of data\n",
        "            e.g. \"text\", \"img\", \"audio\"\n",
        "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
        "            used to save the model\n",
        "    \"\"\"\n",
        "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "\n",
        "\n",
        "    grad_accum_count = args.accum_count\n",
        "    n_gpu = args.world_size\n",
        "\n",
        "    if device_id >= 0:\n",
        "        gpu_rank = int(args.gpu_ranks[device_id])\n",
        "    else:\n",
        "        gpu_rank = 0\n",
        "        n_gpu = 0\n",
        "\n",
        "    print('gpu_rank %d' % gpu_rank)\n",
        "\n",
        "    tensorboard_log_dir = args.model_path\n",
        "\n",
        "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
        "\n",
        "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
        "\n",
        "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
        "\n",
        "    # print(tr)\n",
        "    if (model):\n",
        "        n_params = _tally_parameters(model)\n",
        "        logger.info('* number of parameters: %d' % n_params)\n",
        "\n",
        "    return trainer\n",
        "class Trainer(object):\n",
        "    \"\"\"\n",
        "    Class that controls the training process.\n",
        "\n",
        "    Args:\n",
        "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
        "                to train\n",
        "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
        "               training loss computation\n",
        "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
        "               the optimizer responsible for update\n",
        "            trunc_size(int): length of truncated back propagation through time\n",
        "            shard_size(int): compute loss in shards of this size for efficiency\n",
        "            data_type(string): type of the source input: [text|img|audio]\n",
        "            norm_method(string): normalization methods: [sents|tokens]\n",
        "            grad_accum_count(int): accumulate gradients this many times.\n",
        "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
        "                the object that creates reports, or None\n",
        "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
        "                used to save a checkpoint.\n",
        "                Thus nothing will be saved if this parameter is None\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,  args, model,  optim,\n",
        "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
        "                  report_manager=None):\n",
        "        # Basic attributes.\n",
        "        self.args = args\n",
        "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
        "        self.model = model\n",
        "        self.optim = optim\n",
        "        self.grad_accum_count = grad_accum_count\n",
        "        self.n_gpu = n_gpu\n",
        "        self.gpu_rank = gpu_rank\n",
        "        self.report_manager = report_manager\n",
        "\n",
        "        self.loss = torch.nn.BCELoss(reduction='none')\n",
        "        assert grad_accum_count > 0\n",
        "        # Set model in training mode.\n",
        "        if (model):\n",
        "            self.model.train()\n",
        "\n",
        "    def summary(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
        "        \"\"\" Validate model.\n",
        "            valid_iter: validate data iterator\n",
        "        Returns:\n",
        "            :obj:`nmt.Statistics`: validation loss statistics\n",
        "        \"\"\"\n",
        "        # Set model in validating mode.\n",
        "        def _get_ngrams(n, text):\n",
        "            ngram_set = set()\n",
        "            text_length = len(text)\n",
        "            max_index_ngram_start = text_length - n\n",
        "            for i in range(max_index_ngram_start + 1):\n",
        "                ngram_set.add(tuple(text[i:i + n]))\n",
        "            return ngram_set\n",
        "\n",
        "        def _block_tri(c, p):\n",
        "            tri_c = _get_ngrams(3, c.split())\n",
        "            for s in p:\n",
        "                tri_s = _get_ngrams(3, s.split())\n",
        "                if len(tri_c.intersection(tri_s))>0:\n",
        "                    return True\n",
        "            return False\n",
        "\n",
        "        if (not cal_lead and not cal_oracle):\n",
        "            self.model.eval()\n",
        "        stats = Statistics()\n",
        "\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for batch in test_iter:\n",
        "                src = batch.src\n",
        "                labels = batch.labels\n",
        "                segs = batch.segs\n",
        "                clss = batch.clss\n",
        "                mask = batch.mask\n",
        "                mask_cls = batch.mask_cls\n",
        "\n",
        "\n",
        "                gold = []\n",
        "                pred = []\n",
        "\n",
        "                if (cal_lead):\n",
        "                    selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
        "                elif (cal_oracle):\n",
        "                    selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
        "                                    range(batch.batch_size)]\n",
        "                else:\n",
        "                    sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "                    # loss = self.loss(sent_scores, labels.float())\n",
        "                    # loss = (loss * mask.float()).sum()\n",
        "                    # batch_stats = Statistics(float(loss.cpu().data.numpy()), len(labels))\n",
        "                    # stats.update(batch_stats)\n",
        "\n",
        "                    sent_scores = sent_scores + mask.float()\n",
        "                    sent_scores = sent_scores.cpu().data.numpy()\n",
        "                    selected_ids = np.argsort(-sent_scores, 1)\n",
        "                # selected_ids = np.sort(selected_ids,1)\n",
        "                \n",
        "\n",
        "        return selected_ids\n",
        "\n",
        "\n",
        "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
        "                               report_stats):\n",
        "        if self.grad_accum_count > 1:\n",
        "            self.model.zero_grad()\n",
        "\n",
        "        for batch in true_batchs:\n",
        "            if self.grad_accum_count == 1:\n",
        "                self.model.zero_grad()\n",
        "\n",
        "            src = batch.src\n",
        "            labels = batch.labels\n",
        "            segs = batch.segs\n",
        "            clss = batch.clss\n",
        "            mask = batch.mask\n",
        "            mask_cls = batch.mask_cls\n",
        "\n",
        "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
        "\n",
        "            loss = self.loss(sent_scores, labels.float())\n",
        "            loss = (loss*mask.float()).sum()\n",
        "            (loss/loss.numel()).backward()\n",
        "            # loss.div(float(normalization)).backward()\n",
        "\n",
        "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
        "\n",
        "\n",
        "            total_stats.update(batch_stats)\n",
        "            report_stats.update(batch_stats)\n",
        "\n",
        "            # 4. Update the parameters and statistics.\n",
        "            if self.grad_accum_count == 1:\n",
        "                # Multi GPU gradient gather\n",
        "                if self.n_gpu > 1:\n",
        "                    grads = [p.grad.data for p in self.model.parameters()\n",
        "                             if p.requires_grad\n",
        "                             and p.grad is not None]\n",
        "                    distributed.all_reduce_and_rescale_tensors(\n",
        "                        grads, float(1))\n",
        "                self.optim.step()\n",
        "\n",
        "        # in case of multi step gradient accumulation,\n",
        "        # update only after accum batches\n",
        "        if self.grad_accum_count > 1:\n",
        "            if self.n_gpu > 1:\n",
        "                grads = [p.grad.data for p in self.model.parameters()\n",
        "                         if p.requires_grad\n",
        "                         and p.grad is not None]\n",
        "                distributed.all_reduce_and_rescale_tensors(\n",
        "                    grads, float(1))\n",
        "            self.optim.step()\n",
        "\n",
        "    def _save(self, step):\n",
        "        real_model = self.model\n",
        "        # real_generator = (self.generator.module\n",
        "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
        "        #                   else self.generator)\n",
        "\n",
        "        model_state_dict = real_model.state_dict()\n",
        "        # generator_state_dict = real_generator.state_dict()\n",
        "        checkpoint = {\n",
        "            'model': model_state_dict,\n",
        "            # 'generator': generator_state_dict,\n",
        "            'opt': self.args,\n",
        "            'optim': self.optim,\n",
        "        }\n",
        "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
        "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
        "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
        "        if (not os.path.exists(checkpoint_path)):\n",
        "            torch.save(checkpoint, checkpoint_path)\n",
        "            return checkpoint, checkpoint_path\n",
        "\n",
        "    def _start_report_manager(self, start_time=None):\n",
        "        \"\"\"\n",
        "        Simple function to start report manager (if any)\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            if start_time is None:\n",
        "                self.report_manager.start()\n",
        "            else:\n",
        "                self.report_manager.start_time = start_time\n",
        "\n",
        "    def _maybe_gather_stats(self, stat):\n",
        "        \"\"\"\n",
        "        Gather statistics in multi-processes cases\n",
        "\n",
        "        Args:\n",
        "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
        "                or None (it returns None in this case)\n",
        "\n",
        "        Returns:\n",
        "            stat: the updated (or unchanged) stat object\n",
        "        \"\"\"\n",
        "        if stat is not None and self.n_gpu > 1:\n",
        "            return Statistics.all_gather_stats(stat)\n",
        "        return stat\n",
        "\n",
        "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
        "                               report_stats):\n",
        "        \"\"\"\n",
        "        Simple function to report training stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_training(\n",
        "                step, num_steps, learning_rate, report_stats,\n",
        "                multigpu=self.n_gpu > 1)\n",
        "\n",
        "    def _report_step(self, learning_rate, step, train_stats=None,\n",
        "                     valid_stats=None):\n",
        "        \"\"\"\n",
        "        Simple function to report stats (if report_manager is set)\n",
        "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
        "        \"\"\"\n",
        "        if self.report_manager is not None:\n",
        "            return self.report_manager.report_step(\n",
        "                learning_rate, step, train_stats=train_stats,\n",
        "                valid_stats=valid_stats)\n",
        "\n",
        "    def _maybe_save(self, step):\n",
        "        \"\"\"\n",
        "        Save the model if a model saver is set\n",
        "        \"\"\"\n",
        "        if self.model_saver is not None:\n",
        "            self.model_saver.maybe_save(step)\n",
        "\n",
        "def summary(args, b_list, device_id, pt, step):\n",
        "\n",
        "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "    if (pt != ''):\n",
        "        test_from = pt\n",
        "    else:\n",
        "        test_from = args.test_from\n",
        "    logger.info('Loading checkpoint from %s' % test_from)\n",
        "    checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n",
        "    opt = vars(checkpoint['opt'])\n",
        "    for k in opt.keys():\n",
        "        if (k in model_flags):\n",
        "            setattr(args, k, opt[k])\n",
        "    print(args)\n",
        "\n",
        "    config = BertConfig.from_json_file(args.bert_config_path)\n",
        "    model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
        "    model.load_cp(checkpoint)\n",
        "    model.eval()\n",
        "\n",
        "    test_iter =data_loader.Dataloader(args, _lazy_dataset_loader(b_list),\n",
        "                                  args.batch_size, device,\n",
        "                                  shuffle=False, is_test=True)\n",
        "    trainer = build_trainer(args, device_id, model, None)\n",
        "    result = trainer.summary(test_iter,step)\n",
        "    return result\n",
        "def _tally_parameters(model):\n",
        "    n_params = sum([p.nelement() for p in model.parameters()])\n",
        "    return n_params\n",
        "\n",
        "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus\n",
        "\n",
        "init_logger(args.log_file)\n",
        "device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
        "device_id = 0 if device == \"cuda\" else -1\n",
        "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xji5mhCxcWcg"
      },
      "source": [
        "#4. Input data morp-tokenization workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Diau6__Vhhuo"
      },
      "source": [
        "##your openapi_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moouXLWzclio"
      },
      "outputs": [],
      "source": [
        "openapi_key = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifDaOjcThpEr"
      },
      "source": [
        "##workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAnJ4R8b0hYd"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "import urllib3\n",
        "from glob import glob\n",
        "import collections\n",
        "import six\n",
        "import gc\n",
        "\n",
        "def do_lang ( openapi_key, text ) :\n",
        "    openApiURL = \"http://aiopen.etri.re.kr:8000/WiseNLU\"\n",
        "    requestJson = { \"access_key\": openapi_key, \"argument\": { \"text\": text, \"analysis_code\": \"morp\" } }\n",
        "    http = urllib3.PoolManager()\n",
        "    response = http.request( \"POST\", openApiURL, headers={\"Content-Type\": \"application/json; charset=UTF-8\"}, body=json.dumps(requestJson))\n",
        "    \n",
        "    json_data = json.loads(response.data.decode('utf-8'))\n",
        "    json_result = json_data[\"result\"]\n",
        "    \n",
        "    if json_result == -1:\n",
        "        json_reason = json_data[\"reason\"]\n",
        "        if \"Invalid Access Key\" in json_reason:\n",
        "            logger.info(json_reason)\n",
        "            logger.info(\"Please check the openapi access key.\")\n",
        "            sys.exit()\n",
        "        return \"openapi error - \" + json_reason\n",
        "    else:\n",
        "        json_data = json.loads(response.data.decode('utf-8'))\n",
        "    \n",
        "        json_return_obj = json_data[\"return_object\"]\n",
        "        \n",
        "        return_result = \"\"\n",
        "        json_sentence = json_return_obj[\"sentence\"]\n",
        "        for json_morp in json_sentence:\n",
        "            for morp in json_morp[\"morp\"]:\n",
        "                return_result = return_result+str(morp[\"lemma\"])+\"/\"+str(morp[\"type\"])+\" \"\n",
        "\n",
        "        return return_result\n",
        "class BertData():\n",
        "    def __init__(self, vocab_file_path):\n",
        "        self.tokenizer = Tokenizer(vocab_file_path)\n",
        "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
        "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
        "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
        "\n",
        "    def preprocess(self, src):\n",
        "\n",
        "        if (len(src) == 0):\n",
        "            return None\n",
        "\n",
        "        original_src_txt = [''.join(s) for s in src]\n",
        "\n",
        "\n",
        "        idxs = [i for i, s in enumerate(src) if (len(s) > 0)]\n",
        "\n",
        "        src = [src[i][:20000] for i in idxs]\n",
        "        src = src[:10000]\n",
        "\n",
        "        if (len(src) < 3):\n",
        "            return None\n",
        "\n",
        "        src_txt = [''.join(sent) for sent in src]\n",
        "        text = ' [SEP] [CLS] '.join(src_txt)\n",
        "        src_subtokens = text.split(' ')\n",
        "        src_subtokens = src_subtokens[:510]\n",
        "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
        "\n",
        "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
        "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
        "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
        "        segments_ids = []\n",
        "        for i, s in enumerate(segs):\n",
        "            if (i % 2 == 0):\n",
        "                segments_ids += s * [0]\n",
        "            else:\n",
        "                segments_ids += s * [1]\n",
        "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
        "        labels = None\n",
        "        tgt_txt = None\n",
        "        src_txt = [original_src_txt[i] for i in idxs]\n",
        "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
        "def convert_to_unicode(text):\n",
        "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    elif six.PY2:\n",
        "        if isinstance(text, str):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        elif isinstance(text, unicode):\n",
        "            return text\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "class Tokenizer(object):\n",
        "    def __init__(self, vocab_file_path):\n",
        "        self.vocab_file_path = vocab_file_path\n",
        "        \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "        vocab = collections.OrderedDict()\n",
        "        index = 0\n",
        "        with open(self.vocab_file_path, \"r\", encoding='utf-8') as reader:\n",
        "\n",
        "            while True:\n",
        "                token = convert_to_unicode(reader.readline())\n",
        "                if not token:\n",
        "                    break\n",
        "\n",
        "          ### joonho.lim @ 2019-03-15\n",
        "                if token.find('n_iters=') == 0 or token.find('max_length=') == 0 :\n",
        "\n",
        "                    continue\n",
        "                token = token.split('\\t')[0].strip('_')\n",
        "\n",
        "                token = token.strip()\n",
        "                vocab[token] = index\n",
        "                index += 1\n",
        "        self.vocab = vocab\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            try:\n",
        "                ids.append(self.vocab[token])\n",
        "            except:\n",
        "                ids.append(1)\n",
        "        if len(ids) > 10000:\n",
        "            raise ValueError(\n",
        "                \"Token indices sequence length is longer than the specified maximum \"\n",
        "                \" sequence length for this BERT model ({} > {}). Running this\"\n",
        "                \" sequence through BERT will result in indexing errors\".format(len(ids), 10000)\n",
        "            )\n",
        "        return ids\n",
        "def _lazy_dataset_loader(pt_file):\n",
        "    \n",
        "    dataset = pt_file\n",
        "    \n",
        "    yield dataset\n",
        "def News_to_input(text, openapi_key):\n",
        "    newstemp = do_lang(openapi_key,text)\n",
        "    news = newstemp.split(' ./SF ')[:-1]\n",
        "    bertdata = BertData('/content/001_bert_morp_pytorch/vocab.korean_morp.list')\n",
        "    tmp = bertdata.preprocess(news)\n",
        "    b_data_dict = {\"src\":tmp[0],\n",
        "               \"labels\":[0,1,2],\n",
        "               \"segs\":tmp[2],\n",
        "               \"clss\":tmp[3],\n",
        "               \"src_txt\":tmp[4],\n",
        "               \"tgt_txt\":'hehe'}\n",
        "    b_list = []\n",
        "    b_list.append(b_data_dict) \n",
        "    return b_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nhubPG1c5zt"
      },
      "source": [
        "#5. html for SummaryBot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0HisA9GdU95"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import dash\n",
        "import dash_html_components as html\n",
        "import dash_core_components as dcc\n",
        "import dash_bootstrap_components as dbc\n",
        "from dash.dependencies import Input, Output, State\n",
        "from jupyter_dash import JupyterDash\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer\n",
        "import torch\n",
        "def textbox(text, box=\"other\"):\n",
        "    style = {\n",
        "        \"max-width\": \"55%\",\n",
        "        \"width\": \"max-content\",\n",
        "        \"padding\": \"10px 15px\",\n",
        "        \"border-radius\": \"25px\"\n",
        "    }\n",
        "\n",
        "    if box == \"self\":\n",
        "        style[\"margin-left\"] = \"auto\"\n",
        "        style[\"margin-right\"] = 0\n",
        "\n",
        "        color = \"primary\"\n",
        "        inverse = True\n",
        "\n",
        "    elif box == \"other\":\n",
        "        style[\"margin-left\"] = 0\n",
        "        style[\"margin-right\"] = \"auto\"\n",
        "\n",
        "        color = \"light\"\n",
        "        inverse = False\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Incorrect option for `box`.\")\n",
        "\n",
        "    return dbc.Card(text, style=style, body=True, color=color, inverse=inverse)\n",
        "conversation = html.Div(\n",
        "    style={\n",
        "        \"width\": \"80%\",\n",
        "        \"max-width\": \"800px\",\n",
        "        \"height\": \"70vh\",\n",
        "        \"margin\": \"auto\",\n",
        "        \"overflow-y\": \"auto\",\n",
        "    },\n",
        "    id=\"display-conversation\",\n",
        ")\n",
        "\n",
        "controls = dbc.InputGroup(\n",
        "    style={\"width\": \"80%\", \"max-width\": \"800px\", \"margin\": \"auto\"},\n",
        "    children=[\n",
        "        dbc.Input(id=\"user-input\", placeholder=\"Write to the chatbot...\", type=\"text\"),\n",
        "        dbc.InputGroupAddon(dbc.Button(\"Submit\", id=\"submit\"), addon_type=\"append\",),\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "# Define app\n",
        "app = JupyterDash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "server = app.server\n",
        "\n",
        "\n",
        "# Define Layout\n",
        "app.layout = dbc.Container(\n",
        "    fluid=True,\n",
        "    style={'background-image': 'url(https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=http%3A%2F%2Fcfile21.uf.tistory.com%2Fimage%2F99A30D4B5CB15385210EA0)'},\n",
        "    children=[\n",
        "        html.H1(\"뉴스뚝딱\"),\n",
        "        html.Hr(),\n",
        "        dcc.Store(id=\"store-conversation\", data=\"\"),\n",
        "        conversation,\n",
        "        controls\n",
        "    ],\n",
        ")\n",
        "@app.callback(\n",
        "    Output(\"display-conversation\", \"children\"), [Input(\"store-conversation\", \"data\")]\n",
        ")\n",
        "def update_display(chat_history):\n",
        "    return [\n",
        "        textbox(x, box=\"self\") if i % 2 == 0 else textbox(x, box=\"other\")\n",
        "        for i, x in enumerate(chat_history.split('<token>'))\n",
        "    ]\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    [Output(\"store-conversation\", \"data\"), Output(\"user-input\", \"value\")],\n",
        "    [Input(\"submit\", \"n_clicks\"), Input(\"user-input\", \"n_submit\")],\n",
        "    [State(\"user-input\", \"value\"), State(\"store-conversation\", \"data\")],\n",
        ")\n",
        "def run_chatbot(n_clicks, n_submit, user_input, chat_history):\n",
        "    if n_clicks == 0:\n",
        "        return \"\", \"\"\n",
        "\n",
        "    if user_input is None or user_input == \"\":\n",
        "        return chat_history, \"\"\n",
        "\n",
        "    bot_input_ids = News_to_input(chat_history + user_input, openapi_key)\n",
        "        \n",
        "    chat_history_ids = summary(args, bot_input_ids, -1, '', None)\n",
        "    pred_lst = list(chat_history_ids[0][:3])\n",
        "    final_text = ''\n",
        "    for i,a in enumerate(user_input.split('. ')):\n",
        "        if i in pred_lst:\n",
        "            final_text = final_text+a+'. '\n",
        "    chat_history = user_input + '<token>' +final_text\n",
        "\n",
        "    return chat_history, \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW0GrA7Mg5kr"
      },
      "source": [
        "#6. RUN!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "i_dGcr31dBdp",
        "outputId": "27f634ca-568e-49a0-e60e-bc1e6fb643d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dash app running on:\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, text, element) => {\n    if (!google.colab.kernel.accessAllowed) {\n      return;\n    }\n    element.appendChild(document.createTextNode(''));\n    const url = await google.colab.kernel.proxyPort(port);\n    const anchor = document.createElement('a');\n    anchor.href = url + path;\n    anchor.target = '_blank';\n    anchor.setAttribute('data-href', url + path);\n    anchor.textContent = text;\n    element.appendChild(anchor);\n  })(8050, \"/\", \"http://127.0.0.1:8050/\", window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "app.run_server(mode='external')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3bYOoWzjYSip",
        "d3Uzgqx3cLy0",
        "ifDaOjcThpEr",
        "6nhubPG1c5zt"
      ],
      "name": "Newsdata_summarybot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
